import os
import json
import re
from typing import Dict, List, Optional, Tuple, Set
from sentence_transformers import SentenceTransformer, util
from datetime import datetime
from underthesea import word_tokenize as vn_word_tokenize


SYNONYMS_FILE = os.path.join(os.path.dirname(__file__), "learned_synonyms.json")

VIETNAMESE_SYNONYMS = {
    "xe mÃ¡y": ["xe gáº¯n mÃ¡y", "xe mÃ´ tÃ´", "honda"],
    "quáº£": ["trÃ¡i"],
    "báº¯p": ["ngÃ´"],
    "thÃ¬a": ["muá»—ng"],
    "dÃ©p": ["dÃ©p lÃª", "dÃ©p tÃ´ng"],
    "Ã¡o thun": ["Ã¡o phÃ´ng"],
    "cá»‘c": ["ly"],
    "bÃ¡t": ["chÃ©n", "tÃ´"],
    "xoong": ["ná»“i"],
    "khoai tÃ¢y": ["cá»§ khoai tÃ¢y"],
    "cá»§ cáº£i": ["cÃ  rá»‘t tráº¯ng"],
    "dá»©a": ["thÆ¡m", "khÃ³m"],
    "mÃ­t": ["trÃ¡i mÃ­t"],
    "á»•i": ["trÃ¡i á»•i"],
    "sáº§u riÃªng": ["sáº§u"],
    "chÃ´m chÃ´m": ["trÃ¡i chÃ´m chÃ´m"],
    "cÃ¡ rÃ´": ["cÃ¡ rÃ´ phi"],
    "vá»‹t": ["con vá»‹t"],
    "gÃ ": ["con gÃ "],
    "heo": ["lá»£n"],
    "bÃ²": ["trÃ¢u bÃ²", "con bÃ²"],
    "á»›t": ["trÃ¡i á»›t"],
    "hÃ nh": ["cá»§ hÃ nh", "hÃ nh tÃ¢y"],
    "tá»i": ["cá»§ tá»i"],
    "gá»«ng": ["cá»§ gá»«ng"],
    "nghá»‡": ["cá»§ nghá»‡"],
    "rau muá»‘ng": ["rau"],
    "cáº£i": ["rau cáº£i", "cáº£i xanh"],
    "xÃ  lÃ¡ch": ["rau xÃ  lÃ¡ch", "salad"],

    "Ä‘i": ["bÆ°á»›c", "Ä‘i bá»™", "di chuyá»ƒn"],
    "cháº¡y": ["cháº¡y bá»™", "phi"],
    "nháº£y": ["nháº£y lÃªn", "báº­t nháº£y"],
    "Äƒn": ["dÃ¹ng bá»¯a", "Äƒn uá»‘ng", "xÆ¡i"],
    "uá»‘ng": ["uá»‘ng nÆ°á»›c", "dÃ¹ng"],
    "ngá»§": ["nghá»‰ ngÆ¡i", "ngá»§ nghá»‰", "Ä‘i ngá»§"],
    "thá»©c": ["thá»©c dáº­y", "tá»‰nh dáº­y", "dáº­y"],
    "nÃ³i": ["nÃ³i chuyá»‡n", "trÃ² chuyá»‡n", "trao Ä‘á»•i"],
    "nghe": ["láº¯ng nghe", "nghe tháº¥y"],
    "nhÃ¬n": ["xem", "ngáº¯m", "quan sÃ¡t", "nhÃ¬n tháº¥y"],
    "Ä‘á»c": ["Ä‘á»c sÃ¡ch", "xem"],
    "viáº¿t": ["ghi", "ghi chÃ©p", "chÃ©p"],
    "há»c": ["há»c táº­p", "há»c hÃ nh", "nghiÃªn cá»©u"],
    "dáº¡y": ["giáº£ng dáº¡y", "hÆ°á»›ng dáº«n", "chá»‰ dáº¡y"],
    "lÃ m": ["thá»±c hiá»‡n", "tiáº¿n hÃ nh", "lÃ m viá»‡c"],
    "táº¡o": ["táº¡o ra", "sinh ra", "sÃ¡ng táº¡o"],
    "xÃ¢y": ["xÃ¢y dá»±ng", "kiáº¿n táº¡o"],
    "phÃ¡": ["phÃ¡ há»§y", "há»§y hoáº¡i", "tiÃªu diá»‡t"],
    "má»Ÿ": ["má»Ÿ ra", "khai má»Ÿ"],
    "Ä‘Ã³ng": ["Ä‘Ã³ng láº¡i", "khÃ©p"],
    "báº¯t Ä‘áº§u": ["khá»Ÿi Ä‘áº§u", "báº¯t Ä‘áº§u", "khá»Ÿi sá»±", "má»Ÿ Ä‘áº§u"],
    "káº¿t thÃºc": ["cháº¥m dá»©t", "hoÃ n thÃ nh", "xong"],
    "giÃºp": ["giÃºp Ä‘á»¡", "há»— trá»£", "trá»£ giÃºp"],
    "yÃªu": ["yÃªu thÆ°Æ¡ng", "thÆ°Æ¡ng yÃªu", "máº¿n"],
    "ghÃ©t": ["cÄƒm ghÃ©t", "ghÃ©t bá»"],
    "sá»£": ["sá»£ hÃ£i", "lo sá»£", "hoáº£ng sá»£"],
    "vui": ["vui váº»", "vui má»«ng", "háº¡nh phÃºc"],
    "buá»“n": ["buá»“n bÃ£", "u buá»“n", "Ä‘au buá»“n"],

    "Ä‘áº¹p": ["xinh", "xinh Ä‘áº¹p", "Ä‘áº¹p Ä‘áº½", "tuyá»‡t Ä‘áº¹p"],
    "xáº¥u": ["xáº¥u xÃ­", "tá»“i tá»‡"],
    "tá»‘t": ["tá»‘t Ä‘áº¹p", "tá»‘t lÃ nh", "tuyá»‡t vá»i", "xuáº¥t sáº¯c"],
    "hay": ["thÃº vá»‹", "háº¥p dáº«n"],
    "dá»Ÿ": ["tá»‡", "kÃ©m"],
    "lá»›n": ["to", "to lá»›n", "khá»•ng lá»“", "vÄ© Ä‘áº¡i"],
    "nhá»": ["bÃ©", "nhá» bÃ©", "bÃ© nhá»", "tÃ­ hon"],
    "cao": ["cao lá»›n", "cao rÃ¡o"],
    "tháº¥p": ["lÃ¹n", "tháº¥p bÃ©"],
    "dÃ i": ["dÃ i dáº±ng dáº·c"],
    "ngáº¯n": ["ngáº¯n ngá»§i", "váº¯n"],
    "rá»™ng": ["rá»™ng lá»›n", "bao la", "mÃªnh mÃ´ng"],
    "háº¹p": ["cháº­t", "cháº­t háº¹p"],
    "nhanh": ["nhanh chÃ³ng", "mau", "mau chÃ³ng", "tá»‘c Ä‘á»™"],
    "cháº­m": ["cháº­m cháº¡p", "cháº­m rÃ£i", "thong tháº£"],
    "má»›i": ["má»›i máº»", "tÃ¢n", "tiÃªn tiáº¿n"],
    "cÅ©": ["cÅ© ká»¹", "láº¡c háº­u"],
    "tráº»": ["tráº» trung", "thanh niÃªn"],
    "giÃ ": ["giÃ  cáº£", "lá»›n tuá»•i", "cao tuá»•i"],
    "giÃ u": ["giÃ u cÃ³", "giÃ u sang", "phÃº quÃ½"],
    "nghÃ¨o": ["nghÃ¨o khá»•", "nghÃ¨o nÃ n", "khÃ³ khÄƒn"],
    "khá»e": ["khá»e máº¡nh", "cÆ°á»ng trÃ¡ng", "máº¡nh khá»e"],
    "yáº¿u": ["yáº¿u Ä‘uá»‘i", "á»‘m yáº¿u"],
    "nÃ³ng": ["nÃ³ng ná»±c", "oi bá»©c", "nÃ³ng bá»©c"],
    "láº¡nh": ["láº¡nh láº½o", "giÃ¡ láº¡nh", "bÄƒng giÃ¡"],
    "sÃ¡ng": ["sÃ¡ng sá»§a", "ráº¡ng rá»¡", "chÃ³i lá»i"],
    "tá»‘i": ["tá»‘i tÄƒm", "tÄƒm tá»‘i", "u Ã¡m"],

    "vÃ ": ["cÃ¹ng", "cÃ¹ng vá»›i", "vá»›i"],
    "hoáº·c": ["hay", "hay lÃ ", "hoáº·c lÃ "],
    "nhÆ°ng": ["tuy nhiÃªn", "song", "tháº¿ nhÆ°ng", "máº·c dÃ¹"],
    "vÃ¬": ["bá»Ÿi vÃ¬", "do", "bá»Ÿi", "vÃ¬ ráº±ng"],
    "nÃªn": ["cho nÃªn", "vÃ¬ váº­y", "do Ä‘Ã³", "váº­y nÃªn", "bá»Ÿi váº­y"],
    "náº¿u": ["náº¿u nhÆ°", "giáº£ sá»­", "náº¿u mÃ "],
    "thÃ¬": ["thÃ¬ lÃ "],
    "mÃ ": ["nhÆ°ng mÃ ", "tháº¿ mÃ "],
    "Ä‘á»ƒ": ["Ä‘á»ƒ mÃ ", "nháº±m", "háº§u"],
    "khi": ["lÃºc", "khi mÃ ", "trong khi"],
    "sau": ["sau khi", "sau Ä‘Ã³"],
    "trÆ°á»›c": ["trÆ°á»›c khi", "trÆ°á»›c Ä‘Ã³"],
    "ráº¥t": ["ráº¥t lÃ ", "háº¿t sá»©c", "vÃ´ cÃ¹ng", "cá»±c ká»³"],
    "quÃ¡": ["quÃ¡ má»©c", "quÃ¡ Ä‘á»—i"],
    "láº¯m": ["nhiá»u láº¯m", "ráº¥t nhiá»u"],
    "cÅ©ng": ["cÅ©ng váº­y", "giá»‘ng váº­y"],
    "Ä‘Ã£": ["Ä‘Ã£ tá»«ng", "tá»«ng"],
    "sáº½": ["sáº½ pháº£i"],
    "Ä‘ang": ["Ä‘ang tiáº¿n hÃ nh"],

    "vÃ­ dá»¥": ["thÃ­ dá»¥", "cháº³ng háº¡n", "nhÆ° lÃ ", "cá»¥ thá»ƒ"],
    "Ä‘áº§u tiÃªn": ["trÆ°á»›c háº¿t", "trÆ°á»›c tiÃªn", "thá»© nháº¥t", "Ä‘áº§u tiÃªn lÃ "],
    "thá»© hai": ["tiáº¿p theo", "káº¿ tiáº¿p"],
    "cuá»‘i cÃ¹ng": ["sau cÃ¹ng", "cuá»‘i háº¿t", "sau háº¿t"],
    "quan trá»ng": ["trá»ng yáº¿u", "thiáº¿t yáº¿u", "cáº§n thiáº¿t", "cá»‘t yáº¿u"],
    "phÃ¡t triá»ƒn": ["tiáº¿n bá»™", "phÃ¡t Ä‘áº¡t", "phÃ¡t trÆ°á»Ÿng"],
    "nghiÃªn cá»©u": ["tÃ¬m hiá»ƒu", "kháº£o sÃ¡t", "Ä‘iá»u tra"],
    "phÃ¢n tÃ­ch": ["phÃ¢n giáº£i", "má»• xáº»"],
    "tá»•ng há»£p": ["tá»•ng káº¿t", "khÃ¡i quÃ¡t"],
    "Ä‘Ã¡nh giÃ¡": ["nháº­n xÃ©t", "phÃª bÃ¬nh", "bÃ¬nh giÃ¡"],
    "káº¿t luáº­n": ["káº¿t thÃºc", "tÃ³m láº¡i", "tÃ³m táº¯t"],
    "nguyÃªn nhÃ¢n": ["lÃ½ do", "cÄƒn nguyÃªn"],
    "káº¿t quáº£": ["háº­u quáº£", "thÃ nh quáº£", "káº¿t cá»¥c"],
    "má»¥c Ä‘Ã­ch": ["má»¥c tiÃªu", "Ä‘Ã­ch Ä‘áº¿n"],
    "phÆ°Æ¡ng phÃ¡p": ["cÃ¡ch thá»©c", "biá»‡n phÃ¡p", "phÆ°Æ¡ng thá»©c"],
    "giáº£i phÃ¡p": ["cÃ¡ch giáº£i quyáº¿t", "biá»‡n phÃ¡p"],
    "váº¥n Ä‘á»": ["tháº¯c máº¯c", "cÃ¢u há»i", "bÃ i toÃ¡n"],
    "khÃ¡i niá»‡m": ["Ä‘á»‹nh nghÄ©a", "Ã½ niá»‡m"],
    "lÃ½ thuyáº¿t": ["há»c thuyáº¿t", "lÃ½ luáº­n"],
    "thá»±c hÃ nh": ["thá»±c táº¿", "thá»±c tiá»…n"],

    "viá»‡t nam": ["vn", "nÆ°á»›c viá»‡t nam", "nÆ°á»›c ta", "Ä‘áº¥t nÆ°á»›c viá»‡t nam", "tá»• quá»‘c"],
    "hÃ  ná»™i": ["thá»§ Ä‘Ã´ hÃ  ná»™i", "thá»§ Ä‘Ã´", "hn"],
    "thÃ nh phá»‘ há»“ chÃ­ minh": ["tp hcm", "sÃ i gÃ²n", "hcm", "tphcm", "sg"],
    "Ä‘Ã  náºµng": ["thÃ nh phá»‘ Ä‘Ã  náºµng"],
    "háº£i phÃ²ng": ["thÃ nh phá»‘ háº£i phÃ²ng", "hp"],
    "cáº§n thÆ¡": ["thÃ nh phá»‘ cáº§n thÆ¡"],
    "huáº¿": ["thÃ nh phá»‘ huáº¿", "cá»‘ Ä‘Ã´ huáº¿"],
    
    # ===== IT/TECHNOLOGY TERMS =====
    "machine learning": ["ml", "há»c mÃ¡y", "mÃ¡y há»c", "há»c tá»± Ä‘á»™ng"],
    "artificial intelligence": ["ai", "trÃ­ tuá»‡ nhÃ¢n táº¡o", "trÃ­ thÃ´ng minh nhÃ¢n táº¡o"],
    "deep learning": ["dl", "há»c sÃ¢u"],
    "neural network": ["nn", "máº¡ng nÆ¡-ron", "máº¡ng tháº§n kinh nhÃ¢n táº¡o"],
    "database": ["db", "cÆ¡ sá»Ÿ dá»¯ liá»‡u", "csdd", "dá»¯ liá»‡u"],
    "framework": ["khung lÃ m viá»‡c", "bá»™ khung", "khung pháº§n má»m"],
    "software": ["pháº§n má»m", "á»©ng dá»¥ng", "chÆ°Æ¡ng trÃ¬nh"],
    "hardware": ["pháº§n cá»©ng"],
    "algorithm": ["thuáº­t toÃ¡n", "giáº£i thuáº­t"],
    "programming": ["láº­p trÃ¬nh", "viáº¿t code", "coding"],
    "programming language": ["ngÃ´n ngá»¯ láº­p trÃ¬nh", "ngÃ´n ngá»¯ lt"],
    "python": ["python3", "py", "ngÃ´n ngá»¯ python"],
    "javascript": ["js", "nodejs", "node.js"],
    "data": ["dá»¯ liá»‡u", "sá»‘ liá»‡u", "thÃ´ng tin", "data"],
    "server": ["mÃ¡y chá»§", "server"],
    "client": ["mÃ¡y khÃ¡ch", "client"],
    "internet": ["máº¡ng internet", "internet", "máº¡ng", "in-tÆ¡-nÃ©t"],
    "website": ["web", "trang web", "site"],
    "application": ["á»©ng dá»¥ng", "app", "pháº§n má»m á»©ng dá»¥ng"],
    "function": ["hÃ m", "chá»©c nÄƒng", "tÃ­nh nÄƒng"],
    "variable": ["biáº¿n", "biáº¿n sá»‘"],
    "object": ["Ä‘á»‘i tÆ°á»£ng", "váº­t thá»ƒ", "object"],
    "class": ["lá»›p", "class"],
    "method": ["phÆ°Æ¡ng thá»©c", "method"],
    "api": ["giao diá»‡n láº­p trÃ¬nh á»©ng dá»¥ng", "application programming interface"],
    "cpu": ["bá»™ xá»­ lÃ½ trung tÃ¢m", "vi xá»­ lÃ½", "central processing unit"],
    "ram": ["bá»™ nhá»›", "bá»™ nhá»› táº¡m", "random access memory"],
    "ssd": ["á»• cá»©ng thá»ƒ ráº¯n", "solid state drive"],
    "hdd": ["á»• cá»©ng", "hard disk drive"],
    
    # ===== ACADEMIC TERMS =====
    "Ã½ thá»©c": ["nháº­n thá»©c", "tÆ° duy", "tinh tháº§n"],
    "váº­t cháº¥t": ["thá»ƒ cháº¥t", "váº­t thá»ƒ", "thá»±c thá»ƒ"],
    "phÃ¡p luáº­t": ["luáº­t phÃ¡p", "luáº­t", "quy Ä‘á»‹nh phÃ¡p luáº­t"],
    "báº£o vá»‡": ["báº£o há»™", "che chá»Ÿ", "báº£o vá»‡ quyá»n"],
    "quyá»n lá»£i": ["quyá»n", "lá»£i Ã­ch", "quyá»n vÃ  lá»£i Ã­ch"],
    "xÃ£ há»™i": ["cá»™ng Ä‘á»“ng", "xÃ£ há»™i loÃ i ngÆ°á»i"],
    "quan há»‡ xÃ£ há»™i": ["cÃ¡c má»‘i quan há»‡", "quan há»‡ giá»¯a ngÆ°á»i vá»›i ngÆ°á»i"],
    "Ä‘iá»u chá»‰nh": ["quáº£n lÃ½", "chi phá»‘i", "Ä‘iá»u tiáº¿t"],
    "pháº£n Ã¡nh": ["thá»ƒ hiá»‡n", "biá»ƒu hiá»‡n", "pháº£n chiáº¿u"],
    "khÃ¡ch quan": ["thá»±c táº¿ khÃ¡ch quan", "khÃ¡ch quan", "tháº¿ giá»›i khÃ¡ch quan"],
    "chá»§ quan": ["Ã½ chÃ­ chá»§ quan", "quan Ä‘iá»ƒm cÃ¡ nhÃ¢n"],
    "biá»‡n chá»©ng": ["phÃ©p biá»‡n chá»©ng", "biá»‡n chá»©ng phÃ¡p"],
    "duy váº­t": ["chá»§ nghÄ©a duy váº­t", "duy váº­t luáº­n"],
    "thá»±c tiá»…n": ["thá»±c táº¿", "thá»±c hÃ nh"],
    

    "ngÃ y": ["hÃ´m", "ngÃ y hÃ´m"],
    "hÃ´m nay": ["ngÃ y hÃ´m nay", "bá»¯a nay"],
    "hÃ´m qua": ["ngÃ y hÃ´m qua"],
    "ngÃ y mai": ["mai", "ngÃ y hÃ´m sau"],
    "tuáº§n": ["tuáº§n lá»…"],
    "thÃ¡ng": ["thÃ¡ng nÃ y"],
    "nÄƒm": ["nÄƒm nay"],
    "sÃ¡ng": ["buá»•i sÃ¡ng", "sÃ¡ng sá»›m"],
    "trÆ°a": ["buá»•i trÆ°a", "giá»¯a trÆ°a"],
    "chiá»u": ["buá»•i chiá»u", "xáº¿ chiá»u"],
    "tá»‘i": ["buá»•i tá»‘i", "Ä‘Ãªm"],
    "Ä‘Ãªm": ["ban Ä‘Ãªm", "Ä‘Ãªm khuya"],
    "luÃ´n luÃ´n": ["mÃ£i mÃ£i", "luÃ´n", "lÃºc nÃ o cÅ©ng"],
    "thÆ°á»ng": ["thÆ°á»ng xuyÃªn", "hay"],
    "Ä‘Ã´i khi": ["thá»‰nh thoáº£ng", "thá»‰nh thoáº£ng"],
    "hiáº¿m khi": ["Ã­t khi", "hiáº¿m"],
    
    "Ä‘iá»‡n thoáº¡i": ["Ä‘iá»‡n thoáº¡i di Ä‘á»™ng", "Ä‘t", "phone", "dáº¿"],
    "mÃ¡y tÃ­nh": ["computer", "pc", "laptop", "mÃ¡y vi tÃ­nh"],
    "xe Ä‘áº¡p": ["xe Ä‘áº¡p Ä‘iá»‡n"],
    "Ã´ tÃ´": ["xe hÆ¡i", "xe Ã´ tÃ´", "xe bá»‘n bÃ¡nh"],
    "mÃ¡y bay": ["phi cÆ¡", "tÃ u bay"],
    "tÃ u": ["tÃ u thá»§y", "thuyá»n"],
    "nhÃ ": ["cÄƒn nhÃ ", "ngÃ´i nhÃ ", "nhÃ  cá»­a"],
    "phÃ²ng": ["cÄƒn phÃ²ng", "buá»“ng"],
    "cá»­a": ["cÃ¡nh cá»­a"],
    "bÃ n": ["cÃ¡i bÃ n", "bÃ n há»c"],
    "gháº¿": ["cÃ¡i gháº¿", "gháº¿ ngá»“i"],
    "giÆ°á»ng": ["cÃ¡i giÆ°á»ng", "giÆ°á»ng ngá»§"],
    "sÃ¡ch": ["quyá»ƒn sÃ¡ch", "cuá»‘n sÃ¡ch"],
    "vá»Ÿ": ["quyá»ƒn vá»Ÿ", "táº­p vá»Ÿ"],
    "bÃºt": ["cÃ¡i bÃºt", "viáº¿t"],
    "Ã¡o": ["cÃ¡i Ã¡o", "Ã¡o quáº§n"],
    "quáº§n": ["cÃ¡i quáº§n"],
    "giÃ y": ["Ä‘Ã´i giÃ y", "giÃ y dÃ©p"],
    "mÅ©": ["nÃ³n", "cÃ¡i mÅ©"],
    "kÃ­nh": ["cáº·p kÃ­nh", "kÃ­nh máº¯t"],
    "Ä‘á»“ng há»“": ["cÃ¡i Ä‘á»“ng há»“"],
    "tiá»n": ["tiá»n báº¡c", "kim tiá»n"],
    

    "ngÆ°á»i": ["con ngÆ°á»i", "nhÃ¢n loáº¡i"],
    "bá»‘": ["cha", "ba", "tÃ­a", "bá»"],
    "máº¹": ["mÃ¡", "máº¡", "u"],
    "anh": ["anh trai"],
    "chá»‹": ["chá»‹ gÃ¡i"],
    "em": ["em trai", "em gÃ¡i"],
    "Ã´ng": ["Ã´ng ná»™i", "Ã´ng ngoáº¡i"],
    "bÃ ": ["bÃ  ná»™i", "bÃ  ngoáº¡i"],
    "chÃº": ["bÃ¡c", "cáº­u"],
    "cÃ´": ["dÃ¬", "bÃ¡c gÃ¡i"],
    "báº¡n": ["báº¡n bÃ¨", "ngÆ°á»i báº¡n"],
    "tháº§y": ["tháº§y giÃ¡o", "giÃ¡o viÃªn nam"],
    "cÃ´ giÃ¡o": ["giÃ¡o viÃªn ná»¯", "cÃ´"],
    "há»c sinh": ["sinh viÃªn", "ngÆ°á»i há»c", "há»c viÃªn"],
    "bÃ¡c sÄ©": ["y sÄ©", "tháº§y thuá»‘c"],
    "cÃ´ng nhÃ¢n": ["ngÆ°á»i lao Ä‘á»™ng", "thá»£"],
    "nÃ´ng dÃ¢n": ["ngÆ°á»i nÃ´ng dÃ¢n", "bÃ  con nÃ´ng dÃ¢n"],

    "háº¡nh phÃºc": ["sung sÆ°á»›ng", "vui sÆ°á»›ng", "mÃ£n nguyá»‡n"],
    "Ä‘au khá»•": ["khá»• sá»Ÿ", "thá»‘ng khá»•", "Ä‘au Ä‘á»›n"],
    "lo láº¯ng": ["lo Ã¢u", "bá»“n chá»“n", "lo ngáº¡i"],
    "tá»± hÃ o": ["kiÃªu hÃ£nh", "hÃ£nh diá»‡n"],
    "xáº¥u há»•": ["máº¯c cá»¡", "tháº¹n thÃ¹ng", "ngÆ°á»£ng ngÃ¹ng"],
    "tá»©c giáº­n": ["giáº­n dá»¯", "pháº«n ná»™", "ná»•i giáº­n"],
    "ngáº¡c nhiÃªn": ["kinh ngáº¡c", "báº¥t ngá»", "sá»­ng sá»‘t"],
    "tháº¥t vá»ng": ["chÃ¡n náº£n", "tháº¥t chÃ­"],
 
    "nhiá»u": ["ráº¥t nhiá»u", "Ä‘a sá»‘", "pháº§n lá»›n", "vÃ´ sá»‘"],
    "Ã­t": ["má»™t Ã­t", "chÃºt Ã­t", "thiá»ƒu sá»‘"],
    "táº¥t cáº£": ["toÃ n bá»™", "háº¿t tháº£y", "Ä‘áº§y Ä‘á»§"],
    "má»™t sá»‘": ["má»™t vÃ i", "má»™t Ã­t"],
    "háº§u háº¿t": ["Ä‘a sá»‘", "pháº§n lá»›n", "gáº§n háº¿t"],
    "khÃ´ng cÃ³": ["khÃ´ng há»", "cháº³ng cÃ³"],
    
    # Tech abbreviations
    "ml": ["machine learning"],
    "ai": ["artificial intelligence"],
}




LEARNED_DATA_PATH = os.path.join(os.path.dirname(__file__), "learned_data.json")

class LearningEngine:

    # AI Learning Engine that learns from instructor feedback.
    def __init__(self, model: SentenceTransformer = None):
        self.model = model
        self.patterns_cache: List[Dict] = []  # Cached confirmed patterns
        self.synonyms: Dict[str, Set[str]] = {}  # Learned synonyms
        self.last_reload: datetime = None
        
        # Load base Vietnamese synonyms
        self._load_base_synonyms()
        

        # Load learned patterns from file
        self._load_patterns_from_file()
    
    def _load_base_synonyms(self):
        # Load base Vietnamese synonym knowledge
        for key, values in VIETNAMESE_SYNONYMS.items():
            normalized_key = self._normalize(key)
            if normalized_key not in self.synonyms:
                self.synonyms[normalized_key] = set()
            for v in values:
                self.synonyms[normalized_key].add(self._normalize(v))
            
            # Also add reverse mappings
            for v in values:
                normalized_v = self._normalize(v)
                if normalized_v not in self.synonyms:
                    self.synonyms[normalized_v] = set()
                self.synonyms[normalized_v].add(normalized_key)


    def _load_patterns_from_file(self):
        # Load learned patterns from JSON file
        try:
            if os.path.exists(LEARNED_DATA_PATH):
                with open(LEARNED_DATA_PATH, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        self.patterns_cache = data
                        print(f"[Learning] Loaded {len(self.patterns_cache)} learned patterns from file")
                    else:
                        print("[Learning] Warning: learned_data.json is not a list")
        except Exception as e:
            print(f"[Learning] Could not load patterns file: {e}")

    def _save_patterns_to_file(self):
        # Save learned patterns to JSON file
        try:
            with open(LEARNED_DATA_PATH, 'w', encoding='utf-8') as f:
                json.dump(self.patterns_cache, f, ensure_ascii=False, indent=2)
            print(f"[Learning] Saved {len(self.patterns_cache)} patterns to file")
        except Exception as e:
            print(f"[Learning] Could not save patterns file: {e}")
    
    def _normalize(self, text: str) -> str:
        """Normalize text for comparison"""
        if not text:
            return ""
        text = text.lower().strip()
        text = re.sub(r'[^\w\s\u00C0-\u1EF9]', '', text)
        text = re.sub(r'\s+', ' ', text)
        return text
    
    def _remove_diacritics(self, text: str) -> str:
        # Vietnamese diacritics mapping
        diacritics_map = {
            'Ã ': 'a', 'Ã¡': 'a', 'áº£': 'a', 'Ã£': 'a', 'áº¡': 'a',
            'Äƒ': 'a', 'áº±': 'a', 'áº¯': 'a', 'áº³': 'a', 'áºµ': 'a', 'áº·': 'a',
            'Ã¢': 'a', 'áº§': 'a', 'áº¥': 'a', 'áº©': 'a', 'áº«': 'a', 'áº­': 'a',
            'Ã¨': 'e', 'Ã©': 'e', 'áº»': 'e', 'áº½': 'e', 'áº¹': 'e',
            'Ãª': 'e', 'á»': 'e', 'áº¿': 'e', 'á»ƒ': 'e', 'á»…': 'e', 'á»‡': 'e',
            'Ã¬': 'i', 'Ã­': 'i', 'á»‰': 'i', 'Ä©': 'i', 'á»‹': 'i',
            'Ã²': 'o', 'Ã³': 'o', 'á»': 'o', 'Ãµ': 'o', 'á»': 'o',
            'Ã´': 'o', 'á»“': 'o', 'á»‘': 'o', 'á»•': 'o', 'á»—': 'o', 'á»™': 'o',
            'Æ¡': 'o', 'á»': 'o', 'á»›': 'o', 'á»Ÿ': 'o', 'á»¡': 'o', 'á»£': 'o',
            'Ã¹': 'u', 'Ãº': 'u', 'á»§': 'u', 'Å©': 'u', 'á»¥': 'u',
            'Æ°': 'u', 'á»«': 'u', 'á»©': 'u', 'á»­': 'u', 'á»¯': 'u', 'á»±': 'u',
            'á»³': 'y', 'Ã½': 'y', 'á»·': 'y', 'á»¹': 'y', 'á»µ': 'y',
            'Ä‘': 'd',
            # Uppercase
            'Ã€': 'A', 'Ã': 'A', 'áº¢': 'A', 'Ãƒ': 'A', 'áº ': 'A',
            'Ä‚': 'A', 'áº°': 'A', 'áº®': 'A', 'áº²': 'A', 'áº´': 'A', 'áº¶': 'A',
            'Ã‚': 'A', 'áº¦': 'A', 'áº¤': 'A', 'áº¨': 'A', 'áºª': 'A', 'áº¬': 'A',
            'Ãˆ': 'E', 'Ã‰': 'E', 'áºº': 'E', 'áº¼': 'E', 'áº¸': 'E',
            'ÃŠ': 'E', 'á»€': 'E', 'áº¾': 'E', 'á»‚': 'E', 'á»„': 'E', 'á»†': 'E',
            'ÃŒ': 'I', 'Ã': 'I', 'á»ˆ': 'I', 'Ä¨': 'I', 'á»Š': 'I',
            'Ã’': 'O', 'Ã“': 'O', 'á»Ž': 'O', 'Ã•': 'O', 'á»Œ': 'O',
            'Ã”': 'O', 'á»’': 'O', 'á»': 'O', 'á»”': 'O', 'á»–': 'O', 'á»˜': 'O',
            'Æ ': 'O', 'á»œ': 'O', 'á»š': 'O', 'á»ž': 'O', 'á» ': 'O', 'á»¢': 'O',
            'Ã™': 'U', 'Ãš': 'U', 'á»¦': 'U', 'Å¨': 'U', 'á»¤': 'U',
            'Æ¯': 'U', 'á»ª': 'U', 'á»¨': 'U', 'á»¬': 'U', 'á»®': 'U', 'á»°': 'U',
            'á»²': 'Y', 'Ã': 'Y', 'á»¶': 'Y', 'á»¸': 'Y', 'á»´': 'Y',
            'Ä': 'D',
        }
        result = []
        for char in text:
            result.append(diacritics_map.get(char, char))
        return ''.join(result)
    
    def _tokenize(self, text: str) -> List[str]:
        
        # Vietnamese word tokenization using underthesea.
        normalized = self._normalize(text)
        try:
            # Use underthesea (Stable version 6.8.4 verified on Windows)
            tokens = vn_word_tokenize(normalized, format="text")
            return tokens.split()
        except:
            # Fallback only if strictly necessary
            return normalized.split()
    
    def load_patterns_from_db(self, db_connection) -> int:
        
        # Load confirmed patterns from database.
        try:
            cursor = db_connection.cursor(dictionary=True)
            
            # Get confirmed patterns with score difference
            query = """
                SELECT 
                    al.id,
                    al.student_answer,
                    al.model_answer,
                    al.ai_suggested_score,
                    sa.score as confirmed_score,
                    eq.points as max_points
                FROM ai_logs al
                JOIN student_answers sa ON al.question_id = sa.question_id 
                                        AND al.student_id = sa.student_id
                JOIN submissions s ON sa.submission_id = s.id
                JOIN exam_questions eq ON al.question_id = eq.id
                WHERE s.instructor_confirmed = 1
                  AND sa.status IN ('confirmed', 'graded')
                  AND al.student_answer IS NOT NULL
                  AND al.model_answer IS NOT NULL
            """
            print("[Learning] Executing DB query...")
            cursor.execute(query)
            results = cursor.fetchall()
            print(f"[Learning] DB Query returned {len(results)} rows")
            synonym_candidates = []
            count_new = 0
            
            for row in results:
                pattern = {
                    "student_answer": row["student_answer"],
                    "model_answer": row["model_answer"],
                    "confirmed_score": float(row["confirmed_score"]) if row["confirmed_score"] else 0,
                    "ai_score": float(row["ai_suggested_score"]) if row["ai_suggested_score"] else 0,
                    "max_points": float(row["max_points"]) if row["max_points"] else 1.0
                }
                
                # Check for duplicates efficiently
                is_duplicate = False
                p_stud_norm = self._normalize(pattern["student_answer"])
                p_mod_norm = self._normalize(pattern["model_answer"])

                for existing in self.patterns_cache:
                    if (self._normalize(existing["student_answer"]) == p_stud_norm and
                        self._normalize(existing["model_answer"]) == p_mod_norm):
                        is_duplicate = True
                        break
                
                if not is_duplicate:
                    self.patterns_cache.append(pattern)
                    count_new += 1
                
                # Detect potential synonyms (logic remains same)
                if pattern["max_points"] > 0:
                    ai_ratio = pattern["ai_score"] / pattern["max_points"]
                    confirmed_ratio = pattern["confirmed_score"] / pattern["max_points"]
                    
                    if ai_ratio < 0.6 and confirmed_ratio > 0.8:
                        synonym_candidates.append({
                            "student_words": self._tokenize(row["student_answer"]),
                            "model_words": self._tokenize(row["model_answer"]),
                            "score_diff": confirmed_ratio - ai_ratio
                        })

            # Save merged result to file
            if count_new > 0:
                self._save_patterns_to_file()
                print(f"[Learning] DB Sync: Merged {count_new} new patterns from DB. Total cache: {len(self.patterns_cache)}")
            
            # Learn synonyms from candidates
            print(f"[Learning] Processing {len(synonym_candidates)} synonym candidates...")
            try:
                self._learn_synonyms_from_candidates(synonym_candidates)
            except Exception as e:
                print(f"[Learning] âš ï¸ Error learning synonyms: {e}")
                import traceback
                traceback.print_exc()
            
            cursor.close()
            self.last_reload = datetime.now()
            
            return len(self.patterns_cache)
            
        except Exception as e:
            print(f"[Learning] Error loading patterns: {e}")
            return 0
    
    def _learn_synonyms_from_candidates(self, candidates: List[Dict]):
        # Learn synonyms using underthesea compound words + semantic similarity.
        # Only learns pairs with similarity > 70%.
        new_synonyms_count = 0
        try:
            from app.nlp import get_model
            model = get_model()
        except:
            print("[Learning] âš ï¸ Could not load model for semantic similarity check")
            return
        
        SIMILARITY_THRESHOLD = 0.60  # Learn if similarity > 60% (lowered to catch real synonyms)
        MIN_WORD_LENGTH = 6  # Skip single-syllable words (dÆ°á»›i, giá»i = 4 chars) to avoid false positives
        
        for candidate in candidates:
            student_words = candidate["student_words"]  # Already tokenized with underthesea
            model_words = candidate["model_words"]
            
            # Use underthesea compound words directly (already properly segmented)
            student_phrases = set()
            model_phrases = set()
            
            # Only add words/compounds with length >= MIN_WORD_LENGTH
            for w in student_words:
                # Convert underscore to space for display but keep as single unit
                phrase = w.replace("_", " ")
                if len(phrase) >= MIN_WORD_LENGTH:
                    student_phrases.add(phrase)
            
            for w in model_words:
                phrase = w.replace("_", " ")
                if len(phrase) >= MIN_WORD_LENGTH:
                    model_phrases.add(phrase)
            
            # Find unique words/phrases in each
            unique_student = student_phrases - model_phrases
            unique_model = model_phrases - student_phrases
            
            print(f"[Learning] ðŸ” Checking {len(unique_student)} student Ã— {len(unique_model)} model words (min {MIN_WORD_LENGTH} chars, >65% sim)")
            print(f"[Learning] ðŸ“ Student unique: {list(unique_student)[:10]}")
            print(f"[Learning] ðŸ“ Model unique: {list(unique_model)[:10]}")
            
            if not unique_student or not unique_model:
                print("[Learning] â­ï¸ Skipping: no unique phrases found")
                continue
            
            # Encode all phrases
            student_list = list(unique_student)
            model_list = list(unique_model)
            
            try:
                # Remove diacritics for better similarity matching
                student_nodiacritics = [self._remove_diacritics(s) for s in student_list]
                model_nodiacritics = [self._remove_diacritics(m) for m in model_list]
                
                student_embeddings = model.encode(student_nodiacritics, convert_to_tensor=True)
                model_embeddings = model.encode(model_nodiacritics, convert_to_tensor=True)
                
                # Compute similarity matrix
                similarities = util.cos_sim(student_embeddings, model_embeddings)
                # Debug: Show TOP 5 similarity pairs (regardless of threshold)
                all_pairs = []
                for i, s_phrase in enumerate(student_list):
                    for j, m_phrase in enumerate(model_list):
                        sim = similarities[i][j].item()
                        all_pairs.append((sim, s_phrase, m_phrase))
                
                all_pairs.sort(reverse=True)
                print(f"[Learning] ðŸ“Š Top 5 similarity pairs:")
                for sim, s, m in all_pairs[:5]:
                    status = "âœ…" if sim >= SIMILARITY_THRESHOLD else "âŒ"
                    print(f"[Learning]   {status} {sim:.0%}: '{s}' â†” '{m}'")
                
                # Find pairs with similarity > threshold
                for i, s_phrase in enumerate(student_list):
                    for j, m_phrase in enumerate(model_list):
                        sim = similarities[i][j].item()
                        
                        if sim >= SIMILARITY_THRESHOLD:
                            # Log all pairs that meet threshold
                            print(f"[Learning] ðŸŽ¯ Pair meets threshold ({sim:.0%}): '{s_phrase}' â†” '{m_phrase}'")
                            
                            # Skip if already known
                            if m_phrase in self.synonyms and s_phrase in self.synonyms[m_phrase]:
                                print(f"[Learning] â­ï¸ Skipping (already known): '{s_phrase}' â†” '{m_phrase}'")
                                continue
                            
                            # Skip if same phrase
                            if s_phrase == m_phrase:
                                print(f"[Learning] â­ï¸ Skipping (same phrase): '{s_phrase}'")
                                continue
                            
                            # Add bidirectional mapping
                            if m_phrase not in self.synonyms:
                                self.synonyms[m_phrase] = set()
                            self.synonyms[m_phrase].add(s_phrase)
                            
                            if s_phrase not in self.synonyms:
                                self.synonyms[s_phrase] = set()
                            self.synonyms[s_phrase].add(m_phrase)
                            
                            new_synonyms_count += 1
                            print(f"[Learning] âœ… NEW synonym added: '{s_phrase}' â†” '{m_phrase}'")
                            
            except Exception as e:
                print(f"[Learning] âš ï¸ Error computing similarity: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        if new_synonyms_count > 0:
            # self._save_learned_synonyms() # Disabled file saving
            print(f"[Learning] Found {new_synonyms_count} potential synonym pairs (in-memory only)")
    
    def expand_with_synonyms(self, text: str) -> str:
        words = self._tokenize(text)
        expanded_words = []
        
        for word in words:
            expanded_words.append(word)
            # Add synonyms if available
            if word in self.synonyms:
                for syn in self.synonyms[word]:
                    if syn not in expanded_words:
                        expanded_words.append(syn)
        
        return " ".join(expanded_words)
    
    def find_similar_pattern(self, student_answer: str, model_answer: str, 
                            threshold: float = 0.88) -> Optional[Dict]:
        if not self.patterns_cache:
            return None
        
        student_norm = self._normalize(student_answer)
        model_norm = self._normalize(model_answer)
        
        # Encode current answer if model is available
        student_emb = None
        if self.model:
            student_emb = self.model.encode(student_norm, normalize_embeddings=True)
        
        best_match = None
        best_sim = 0.0
        
        for pattern in self.patterns_cache:
            # Check if model answer matches (same question context)
            pattern_model_norm = self._normalize(pattern["model_answer"])
            if pattern_model_norm != model_norm:
                continue
            
            # Compare student answers
            pattern_student_norm = self._normalize(pattern["student_answer"])
            
            # Quick exact match check
            if student_norm == pattern_student_norm:
                return {
                    "confirmed_score": pattern["confirmed_score"],
                    "confidence": 1.0,
                    "match_type": "exact"
                }
            
            # Semantic similarity check (only if model available)
            if self.model and student_emb is not None:
                pattern_emb = self.model.encode(pattern_student_norm, normalize_embeddings=True)
                sim = float(util.cos_sim(student_emb, pattern_emb).item())
                
                if sim > best_sim and sim >= threshold:
                    best_sim = sim
                    best_match = pattern
        
        if best_match:
            return {
                "confirmed_score": best_match["confirmed_score"],
                "confidence": round(best_sim, 2),
                "match_type": "semantic"
            }
        
        return None

    def add_learned_pattern(self, student_answer: str, model_answer: str, 
                           confirmed_score: float, max_points: float = 1.0) -> None:
        pattern = {
            "student_answer": student_answer,
            "model_answer": model_answer,
            "confirmed_score": float(confirmed_score),
            "ai_score": 0.0, # Not needed for matching
            "max_points": float(max_points)
        }
        
        # Check if already exists to avoid duplicates
        stud_norm = self._normalize(student_answer)
        mod_norm = self._normalize(model_answer)
        
        updated = False
        for p in self.patterns_cache:
            if (self._normalize(p["student_answer"]) == stud_norm and 
                self._normalize(p["model_answer"]) == mod_norm):
                # Update existing score
                p["confirmed_score"] = float(confirmed_score)
                updated = True
                print(f"[Learning] Updated existing pattern in cache: '{student_answer[:20]}...'")
                break

        if not updated:
            self.patterns_cache.append(pattern)
            print(f"[Learning] Added new live pattern to cache: '{student_answer[:20]}...' (Score: {confirmed_score})")
            
        # Save to file immediately
        self._save_patterns_to_file()
    
    def get_stats(self) -> Dict:
        #Get learning statistics
        return {
            "total_patterns": len(self.patterns_cache),
            "total_synonym_groups": len(self.synonyms),
            "base_synonyms": len(VIETNAMESE_SYNONYMS),
            "learned_synonyms": len(self.synonyms) - len(VIETNAMESE_SYNONYMS),
            "last_reload": self.last_reload.isoformat() if self.last_reload else None
        }


# Singleton instance
_learning_engine: Optional[LearningEngine] = None


def get_learning_engine(model: SentenceTransformer = None) -> LearningEngine:
    """Get or create the singleton learning engine"""
    global _learning_engine
    if _learning_engine is None:
        _learning_engine = LearningEngine(model)
    elif model is not None and _learning_engine.model is None:
        _learning_engine.model = model
    return _learning_engine
